{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 6: Image search using CLIP (Pre-trained version)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/solution_notebooks/06_CLIP_image_search_pretrained.ipynb)\n",
    "\n",
    "This lab will download encoded images and images indices from google drive to try out the interactive UI\n",
    "\n",
    "* Dataset ref: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\n",
    "    * images in dataset use in this notebook are resized images from H&M personalized fashion recommendations (resize to 100 * 100 pixel)\n",
    "    * contains 100k+ images\n",
    "    * mounted on google drive: https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\n",
    "\n",
    "* Objectives\n",
    "    * find and recommend clothes for customer using image/text search\n",
    "\n",
    "* Notes\n",
    "    * openai-clip: https://github.com/openai/CLIP\n",
    "    * faiss: https://github.com/facebookresearch/faiss/wiki\n",
    "    * please change runtime on google colab for faster computation\n",
    "    * for direct trained version [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/solution_notebooks/05_CLIP_image_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gdown\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available runtime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": \n",
    "  ! pip install faiss-gpu \n",
    "else:\n",
    "  ! pip install faiss-cpu \n",
    "\n",
    "print(\"Now running with \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract resize images for recommendations\n",
    "url = \"https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)\n",
    "\n",
    "# extract dataset zipped file\n",
    "path =  op.join(os.getcwd(),\"h-and-m-resize-image-zip/h-and-m-resize-image.zip\")\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(path, 'r') as zip:\n",
    "\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    %time zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download encoded images from shared google drive\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/132_YbF_cSFZMGesD0wLU_3CMoe0LOtI2?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_storage = np.load(\"/encoded_embeddings/h-and-m-CLIP-image-embeddings.npy\")\n",
    "indices_file = open(\"/encoded_embeddings/item_path.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
