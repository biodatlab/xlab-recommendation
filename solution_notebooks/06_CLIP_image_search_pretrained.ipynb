{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 6: Image search using CLIP (Pre-trained version)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/solution_notebooks/06_CLIP_image_search_pretrained.ipynb)\n",
    "\n",
    "This lab will download encoded images and images indices from google drive to try out the interactive UI\n",
    "\n",
    "* Dataset ref: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\n",
    "    * images in dataset use in this notebook are resized images from H&M personalized fashion recommendations (resize to 100 * 100 pixel)\n",
    "    * contains 100k+ images\n",
    "    * mounted on google drive: https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\n",
    "\n",
    "* Objectives\n",
    "    * find and recommend clothes for customer using image/text search\n",
    "\n",
    "* Notes\n",
    "    * openai-clip: https://github.com/openai/CLIP\n",
    "    * faiss: https://github.com/facebookresearch/faiss/wiki\n",
    "    * please change runtime on google colab for faster computation\n",
    "    * for direct trained version [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/solution_notebooks/05_CLIP_image_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gdown\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available runtime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": \n",
    "  ! pip install faiss-gpu \n",
    "else:\n",
    "  ! pip install faiss-cpu \n",
    "\n",
    "print(\"Now running with \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract resize images for recommendations\n",
    "url = \"https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)\n",
    "\n",
    "# extract dataset zipped file\n",
    "path =  op.join(os.getcwd(),\"h-and-m-resize-image-zip/h-and-m-resize-image.zip\")\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(path, 'r') as zip:\n",
    "\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    %time zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download encoded images from shared google drive\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/132_YbF_cSFZMGesD0wLU_3CMoe0LOtI2?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset_path = op.join(os.getcwd(), \"resized_images/\")\n",
    "\n",
    "# create list of all filename in dataset folder\n",
    "\n",
    "all_folder_path = os.listdir(dataset_path)\n",
    "all_folder_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_storage = np.load(\n",
    "    op.join(os.getcwd(), \"encoded_embeddings/h-and-m-CLIP-image-embeddings.npy\")\n",
    ")\n",
    "indices_file = open(op.join(os.getcwd(), \"encoded_embeddings/item_path.txt\"), \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = indices_file.readlines()\n",
    "images_path = [path.strip(\"\\n\") for path in images_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings vector using FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(\n",
    "    512\n",
    ")  # dimension of 1 embedding decoded from CLIP model is 512\n",
    "index.add(embeddings_storage)\n",
    "print(index.ntotal)  # number of images embeddings store in dataset vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend from images\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def recommend_similar_image(image_path):\n",
    "    print(f\"get image path {image_path}\")\n",
    "\n",
    "    test_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = (\n",
    "            model.encode_image(test_image).numpy(force=True)[0].astype(\"float32\")\n",
    "        )\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 4  # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings, k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "\n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [\n",
    "        (\n",
    "            Image.open(op.join(dataset_path, images_path[image_index[0][i]])),\n",
    "            f\"Recommended Rank {i+1}\",\n",
    "        )\n",
    "        for i in range(k)\n",
    "    ]\n",
    "    return recommended_images\n",
    "\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend from text\n",
    "\n",
    "\n",
    "def recommend_similar_image(text):\n",
    "    # print(f\"get image path {image_path}\")\n",
    "    # original_image = Image.open(image_path).resize((100,100))\n",
    "    # test_image = preprocess(original_image).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model.encode_text(text).numpy(force=True)[0].astype(\"float32\")\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 10  # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings, k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "\n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [\n",
    "        (\n",
    "            Image.open(op.join(dataset_path, images_path[image_index[0][i]])),\n",
    "            f\"Recommended Rank {i+1}\",\n",
    "        )\n",
    "        for i in range(k)\n",
    "    ]\n",
    "    return recommended_images\n",
    "\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Textbox(),\n",
    "    outputs=gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
