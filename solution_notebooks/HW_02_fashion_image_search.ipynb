{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02: Using CLIP to embed product images and recommend similar items\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/main/solution_notebooks/HW_02_fashion_image_search.ipynb)\n",
    "\n",
    "* Dataset: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset\n",
    "* Interface with Kaggle API: https://www.kaggle.com/discussions/general/74235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the \"interface with Kaggle API\" link, upload kaggle.json\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test access kaggle dataset \n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "\n",
    "!kaggle datasets download -d paramaggarwal/fashion-product-images-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset zip file\n",
    "\n",
    "!unzip fashion-product-images-small.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SOLUTION: CLIP Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available runtime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": \n",
    "  ! pip install faiss-gpu \n",
    "else:\n",
    "  ! pip install faiss-cpu \n",
    "\n",
    "print(\"Now running with \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = op.join(os.getcwd(), \"myntradataset/images\")\n",
    "all_images_path = os.listdir(op.join(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_path.sort()\n",
    "print(all_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_storage = np.zeros((len(all_images_path), 512), dtype=np.float32)\n",
    "\n",
    "file_counter = 0\n",
    "\n",
    "for images in tqdm(all_images_path):\n",
    "    with torch.no_grad():\n",
    "        image = (\n",
    "            preprocess(Image.open(op.join(os.getcwd(), dataset_path, images)))\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        embeddings_storage[file_counter] = np.array(\n",
    "            model.encode_image(image).numpy(force=True)[0].astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        file_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings vector using FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(\n",
    "    512\n",
    ")  # dimension of 1 embedding decoded from CLIP model is 512\n",
    "index.add(embeddings_storage)\n",
    "\n",
    "# add embeddings into faiss vector\n",
    "\n",
    "print(index.ntotal)  # number of images embeddings store in dataset vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend from images\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def recommend_similar_image(image_path):\n",
    "    print(f\"get image path {image_path}\")\n",
    "\n",
    "    test_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = (\n",
    "            model.encode_image(test_image).numpy(force=True)[0].astype(\"float32\")\n",
    "        )\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 4  # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings, k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "\n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [\n",
    "        (\n",
    "            Image.open(op.join(dataset_path, all_images_path[image_index[0][i]])),\n",
    "            f\"Recommended Rank {i+1}\",\n",
    "        )\n",
    "        for i in range(k)\n",
    "    ]\n",
    "    return recommended_images\n",
    "\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.Gallery(),\n",
    ").launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
