{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5: Image search using CLIP\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/main/solution_notebooks/05_CLIP_image_search.ipynb)\n",
    "\n",
    "* Dataset ref: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\n",
    "    * รูปภาพที่ใช้เป็น dataset ภายใน notebook นี้ เป็นรูปจาก H&M personalized fashion recommendations ที่สุ่มเลือกมา แล้วนำไป resize\n",
    "    * ซึ่งฝากไว้บน google drive: \n",
    "\n",
    "* Objectives\n",
    "    * ช่วยลูกค้าค้นหาเสื้อผ้าในร้าน/website จากรูปที่ลูกค้าให้มา\n",
    "\n",
    "* Notes\n",
    "    * openai-clip: https://github.com/openai/CLIP\n",
    "    * faiss: https://github.com/facebookresearch/faiss/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install faiss-cpu\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import clip\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set running device to cpu\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see openai-clip available pre-train model\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-select dataset from shared google drive\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  op.join(os.getcwd(),\"h-and-m-resize-image-zip/h-and-m-resize-image.zip\")\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(path, 'r') as zip:\n",
    "\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    %time zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset_path = op.join(os.getcwd(), \"resize_image/\")\n",
    "\n",
    "# create list of all filename in dataset folder\n",
    "\n",
    "all_folder_path = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check folder path\n",
    "\n",
    "print(all_folder_pathl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings vector using FAISS\n",
    "\n",
    "index = faiss.IndexFlatL2(512) # dimension of 1 embedding decoded from CLIP model is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode dataset & store images name\n",
    "\n",
    "images_path = []\n",
    "\n",
    "for folder in tqdm(all_image_folder):\n",
    "    folder_path = op.join(dataset_path,folder)\n",
    "    temp_all_image_name = os.listdir(folder_path)\n",
    "    with torch.no_grad():\n",
    "      for image_name in temp_all_image_name:\n",
    "          image = preprocess(Image.open(op.join(folder_path,image_name))).unsqueeze(0).to(device)\n",
    "          index.add(np.array([model.encode_image(image).numpy(force=True)[0].astype('float32')]))\n",
    "          images_path.append(op.join(folder_path,image_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings into faiss vector\n",
    "\n",
    "print(index.ntotal) # number of images embeddings store in dataset vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def recommend_similar_image(image_path):\n",
    "    print(f\"get image path {image_path}\")\n",
    "\n",
    "    test_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model.encode_image(test_image).numpy(force=True)[0].astype('float32')\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 4 # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings,k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "    \n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [(Image.open(dataset_path + all_image_name[image_index[0][i]]), f\"Recommended Rank {i+1}\") for i in range(k)]\n",
    "    return recommended_images\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs= gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB1-XXQFwi6-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
