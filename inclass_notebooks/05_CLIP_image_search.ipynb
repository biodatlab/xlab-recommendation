{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5: Image search using CLIP\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/main/inclass_notebooks/05_CLIP_image_search.ipynb)\n",
    "\n",
    "* Dataset ref: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\n",
    "    * images in dataset use in this notebook are resized images from H&M personalized fashion recommendations (resize to 100 * 100 pixel)\n",
    "    * contains 100k+ images\n",
    "    * mounted on google drive: https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\n",
    "\n",
    "* Objectives\n",
    "    * find and recommend clothes for customer using image/text search\n",
    "\n",
    "* Notes\n",
    "    * openai-clip: https://github.com/openai/CLIP\n",
    "    * faiss: https://github.com/facebookresearch/faiss/wiki\n",
    "    * please change runtime on google colab for faster computation\n",
    "    * try out saved embeddings at [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/solution_notebooks/06_CLIP_image_search_pretrained.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available runtime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": \n",
    "  ! pip install faiss-gpu \n",
    "else:\n",
    "  ! pip install faiss-cpu \n",
    "\n",
    "print(\"Now running with \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see openai-clip available pre-train model\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download resized image dataset from shared google drive\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1jX1hasS6HysjEuKG0ucmTxdndB03uliJ?usp=sharing\"\n",
    "gdown.download_folder(url, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset zipped file\n",
    "path =  op.join(os.getcwd(),\"h-and-m-resize-image-zip/h-and-m-resize-image.zip\")\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(path, 'r') as zip:\n",
    "\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    %time zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset_path = op.join(os.getcwd(), \"resized_images/\")\n",
    "\n",
    "# create list of all filename in dataset folder\n",
    "\n",
    "all_folder_path = os.listdir(dataset_path)\n",
    "all_folder_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check folder path\n",
    "\n",
    "print(all_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of files in all sub-folders to allocate numpy array for saving embeddings\n",
    "\n",
    "num_file = 0\n",
    "images_path = []\n",
    "\n",
    "for folder in all_folder_path:\n",
    "    temp_all_image_name = os.listdir(op.join(dataset_path, folder))\n",
    "    temp_all_image_name.sort()\n",
    "    for image in temp_all_image_name:\n",
    "        if op.isfile(op.join(dataset_path, folder, image)):\n",
    "            images_path.append(op.join(folder, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: allocate memory for embeddings array with shape of (number_of_files,512), Hint: create arrays of zeros\n",
    "\n",
    "embeddings_storage = ...\n",
    "\n",
    "# encode dataset & store images name\n",
    "file_counter = 0\n",
    "\n",
    "for path in tqdm(images_path):\n",
    "    with torch.no_grad():\n",
    "        image = (\n",
    "            preprocess(Image.open(op.join(dataset_path, path))).unsqueeze(0).to(device)\n",
    "        )\n",
    "        # TODO: encode images using CLIP model, Hint: use previous define \"model\" \n",
    "        embeddings_storage[file_counter] = np.array(...)\n",
    "        file_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings vector using FAISS\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(\n",
    "    512\n",
    ")  # dimension of 1 embedding decoded from CLIP model is 512\n",
    "index.add(embeddings_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings into faiss vector\n",
    "\n",
    "print(index.ntotal)  # number of images embeddings store in dataset vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend from images\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def recommend_similar_image(image_path):\n",
    "    print(f\"get image path {image_path}\")\n",
    "\n",
    "    test_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = (\n",
    "            model.encode_image(test_image).numpy(force=True)[0].astype(\"float32\")\n",
    "        )\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 4  # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings, k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "\n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [\n",
    "        (\n",
    "            Image.open(op.join(dataset_path, images_path[image_index[0][i]])),\n",
    "            f\"Recommended Rank {i+1}\",\n",
    "        )\n",
    "        for i in range(k)\n",
    "    ]\n",
    "    return recommended_images\n",
    "\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend from text\n",
    "\n",
    "\n",
    "def recommend_similar_image(text):\n",
    "    # print(f\"get image path {image_path}\")\n",
    "    # original_image = Image.open(image_path).resize((100,100))\n",
    "    # test_image = preprocess(original_image).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model.encode_text(text).numpy(force=True)[0].astype(\"float32\")\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    k = 10  # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings, k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "\n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [\n",
    "        (\n",
    "            Image.open(op.join(dataset_path, images_path[image_index[0][i]])),\n",
    "            f\"Recommended Rank {i+1}\",\n",
    "        )\n",
    "        for i in range(k)\n",
    "    ]\n",
    "    return recommended_images\n",
    "\n",
    "\n",
    "example_path = []\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Textbox(),\n",
    "    outputs=gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB1-XXQFwi6-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
