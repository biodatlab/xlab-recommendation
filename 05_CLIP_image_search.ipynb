{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5: Image search using CLIP\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/biodatlab/xlab-recommendation/blob/notebook/05_CLIP_image_search.ipynb)\n",
    "\n",
    "* Dataset ref: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\n",
    "    * รูปภาพที่ใช้เป็น dataset ภายใน notebook นี้ เป็นรูปจาก H&M personalized fashion recommendations ที่สุ่มเลือกมา แล้วนำไป resize\n",
    "    * ซึ่งฝากไว้บน google drive: \n",
    "\n",
    "* Objectives\n",
    "    * ช่วยลูกค้าค้นหาเสื้อผ้าในร้าน/website จากรูปที่ลูกค้าให้มา\n",
    "\n",
    "* Notes\n",
    "    * openai-clip: https://github.com/openai/CLIP\n",
    "    * faiss: https://github.com/facebookresearch/faiss/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "\n",
    "! pip install torch ftfy regex tqdm numpy\n",
    "! pip install openai-clip\n",
    "! pip install gradio\n",
    "! pip install faiss-cpu\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential library\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import clip\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set running device to cpu\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see openai-clip available pre-train model\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Vit-B/32 model\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-select dataset from shared google drive\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1eFmm2TUrsPUzPWRP_sE3CnJc6g-La3rC?usp=drive_link\"\n",
    "gdown.download_folder(url, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset_path = op.join(os.getcwd(), \"HM_sample_dataset/\")\n",
    "\n",
    "# create list of all filename in dataset folder\n",
    "\n",
    "all_image_name = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings vector using FAISS\n",
    "\n",
    "index = faiss.IndexFlatL2(512) # dimension of 1 embedding decoded from CLIP model is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode dataset\n",
    "dataset_embeddings = []\n",
    "\n",
    "for image_name in tqdm(all_image_name):\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(op.join(dataset_path,image_name))).unsqueeze(0).to(device)\n",
    "        dataset_embeddings.append(model.encode_image(image).numpy(force=True)[0].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings into faiss vector\n",
    "\n",
    "dataset_embeddings_np = np.array(dataset_embeddings) # change to numpy array for FAISS \n",
    "index.add(dataset_embeddings_np)\n",
    "\n",
    "print(index.ntotal) # number of images embeddings store in dataset vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def recommend_similar_image(image_path):\n",
    "    print(f\"get image path {image_path}\")\n",
    "\n",
    "    test_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model.encode_image(test_image).numpy(force=True)[0].astype('float32')\n",
    "        test_embeddings = np.array([test_embeddings])\n",
    "\n",
    "    recommend_number = 4 # number of recommendations\n",
    "    square_distance, image_index = index.search(test_embeddings,k)\n",
    "    print(image_index)\n",
    "    print(square_distance)\n",
    "    \n",
    "    print(\"Opening Images...\")\n",
    "    recommended_images = [(Image.open(dataset_path + all_image_name[image_index[0][i]]), f\"Recommended Rank {i+1}\") for i in range(recommend_number)]\n",
    "    return recommended_images\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_similar_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs= gr.Gallery(),\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB1-XXQFwi6-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
